# âš¡ Hybrid SoC Predictor with Error Correction (BiGRU + Transformer)

This repository implements a **two-stage deep learning pipeline** for **battery State of Charge (SoC)** prediction using **time-series modeling** and **post-prediction error correction**.  
It combines the sequential learning power of a **Bidirectional GRU (BiGRU)** with the context-awareness of a **Transformer**, resulting in a highly robust prediction architecture capable of handling **low-SoC edge cases** and improving **prediction stability**.

---

## ğŸš€ Key Features

- **Dual-Stage Learning Framework**
  - **Stage 1:** A **BiGRU with Multi-Head Attention** predicts the base SoC curve.
  - **Stage 2:** A **Transformer-based error correction model** learns to predict residual errors from Stage 1 and adjusts outputs for improved accuracy.

- **Error-Aware Weighted Loss Function**
  - Assigns higher penalty to **low SoC ranges** where predictive accuracy is most critical.

- **Dynamic Data Handling**
  - Automatically loads or **generates dummy datasets** if source files are missing, ensuring end-to-end reproducibility.

- **Robust Feature Engineering**
  - Incorporates **lagged**, **differential**, and **rolling statistical** features for richer time-series representation.

- **Self-Adaptive Training**
  - Includes **EarlyStopping** and **ReduceLROnPlateau** callbacks to prevent overfitting and dynamically tune learning rates.

- **Comprehensive Visualization**
  - Generates detailed **SoC vs. time**, **error correction**, and **smoothed Savitzkyâ€“Golay filtered** performance plots.

---

## ğŸ§© Architecture Overview

Input CSVs
â”‚
â”œâ”€â”€ Data Loading & Cleaning
â”‚
â”œâ”€â”€ Feature Engineering
â”‚ â”œâ”€ Lag Features
â”‚ â”œâ”€ Derivative Features
â”‚ â”œâ”€ Rolling Mean/Std/Min/Max
â”‚
â”œâ”€â”€ Sequence Construction (Sliding Window)
â”‚
â”œâ”€â”€ Stage 1: BiGRU + Multi-Head Attention
â”‚ â””â”€ Weighted MSE Loss (Low-SoC Focused)
â”‚
â”œâ”€â”€ Stage 2: Transformer (Error Correction)
â”‚ â””â”€ Learns Residual Errors from Stage 1
â”‚
â”œâ”€â”€ Evaluation Metrics
â”‚ â”œâ”€ MSE / MAE / RÂ²
â”‚ â”œâ”€ % Improvement after Correction
â”‚
â””â”€â”€ Visualization & Reporting


---

## ğŸ§  Core Concepts

### ğŸ”¹ BiGRU with Multi-Head Attention
Captures both **past and future dependencies** while refining attention on the most relevant temporal features.

### ğŸ”¹ Transformer for Error Correction
Learns **systematic residual patterns** (bias, drift, temporal error trends) from BiGRU predictions and corrects them.

### ğŸ”¹ Weighted Loss Function
Gives **extra importance to low SoC** values to reduce underestimation risk in critical discharge zones:
```python
low_soc_threshold = 25.0
high_error_weight = 15.0

ğŸ“Š Performance Metrics

The script outputs a full comparison of pre- and post-correction performance:

Metric	BiGRU (Base)	BiGRU + Transformer (Corrected)	 % Improvement
MSE	Lower = Better	             âœ… Improved	            âœ” Significant
MAE	Lower = Better	             âœ… Improved	            âœ” Noticeable
RÂ²	Higher = Better	             âœ… Improved	            âœ” Higher Fit

Additionally, the script prints standard deviation and mean of residual errors for stability analysis.

.

ğŸ§° Requirements

Run "pip install -r requirements.txt" to install all the required dependencies.

ğŸ“‚ File Structure
project/
â”‚
â”œâ”€â”€ dataset/
â”‚   â”œâ”€â”€ 2019.1.13.xlsx     # Base training dataset (normal SoC)
â”‚   â”œâ”€â”€ 2019.1.21.xlsx     # Test dataset (contains low SoC)
â”‚
â”œâ”€â”€ hybrid_soc_predictor_with_error_correction.py
â””â”€â”€ README.md

If datasets are missing, the script will automatically generate synthetic data to preserve functionality.

ğŸ§ª Usage
â–¶ï¸ Run the training and evaluation:
python hybrid_soc_predictor_with_error_correction.py


The script will:

1. Load (or generate) training and test data.

2. Perform feature engineering.

3. Train the BiGRU model.

4. Train the Transformer on BiGRU residuals.

5. Evaluate and visualize results.


ğŸ“ˆ Visualization Outputs

The notebook automatically produces:

1. Actual vs Predicted SoC

2. Error Prediction Over Time

3. Smoothed SoC Curve (Savitzkyâ€“Golay Filter)

4. Comparative performance logs

âš™ï¸ Customization

You can tweak key hyperparameters:

Parameter	        Default	        Description
TIME_STEPS	          15	   Sliding window length
learning_rate	    0.0005	   Learning rate for Adam
high_error_weight     15.0	   Emphasis on low SoC penalty
num_heads	          4	       Transformer attention heads
dropout_rate	  0.4 - 0.5	   Regularization strength

ğŸ§© Error Correction Impact

The second stage effectively reduces prediction drift and enhances robustness near critical discharge points, quantified via:

Î”MSE (Mean Squared Error reduction)

Î”MAE (Mean Absolute Error reduction)

Î”RÂ² (Improvement in explained variance)

ğŸ“œ License

This project is released under the MIT License.
Feel free to use, modify, and extend it for research or production use.